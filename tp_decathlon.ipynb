{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensions = (41, 13)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>100m</th>\n",
       "      <th>Longueur</th>\n",
       "      <th>Poids</th>\n",
       "      <th>Hauteur</th>\n",
       "      <th>400m</th>\n",
       "      <th>110m H</th>\n",
       "      <th>Disque</th>\n",
       "      <th>Perche</th>\n",
       "      <th>Javelot</th>\n",
       "      <th>1500m</th>\n",
       "      <th>Classement</th>\n",
       "      <th>Points</th>\n",
       "      <th>Competition</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Sebrle</th>\n",
       "      <td>10.85</td>\n",
       "      <td>7.84</td>\n",
       "      <td>16.36</td>\n",
       "      <td>2.12</td>\n",
       "      <td>48.36</td>\n",
       "      <td>14.05</td>\n",
       "      <td>48.72</td>\n",
       "      <td>5.00</td>\n",
       "      <td>70.52</td>\n",
       "      <td>280.01</td>\n",
       "      <td>1</td>\n",
       "      <td>8893</td>\n",
       "      <td>JO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Clay</th>\n",
       "      <td>10.44</td>\n",
       "      <td>7.96</td>\n",
       "      <td>15.23</td>\n",
       "      <td>2.06</td>\n",
       "      <td>49.19</td>\n",
       "      <td>14.13</td>\n",
       "      <td>50.11</td>\n",
       "      <td>4.90</td>\n",
       "      <td>69.71</td>\n",
       "      <td>282.00</td>\n",
       "      <td>2</td>\n",
       "      <td>8820</td>\n",
       "      <td>JO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Karpov</th>\n",
       "      <td>10.50</td>\n",
       "      <td>7.81</td>\n",
       "      <td>15.93</td>\n",
       "      <td>2.09</td>\n",
       "      <td>46.81</td>\n",
       "      <td>13.97</td>\n",
       "      <td>51.65</td>\n",
       "      <td>4.60</td>\n",
       "      <td>55.54</td>\n",
       "      <td>278.11</td>\n",
       "      <td>3</td>\n",
       "      <td>8725</td>\n",
       "      <td>JO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Macey</th>\n",
       "      <td>10.89</td>\n",
       "      <td>7.47</td>\n",
       "      <td>15.73</td>\n",
       "      <td>2.15</td>\n",
       "      <td>48.97</td>\n",
       "      <td>14.56</td>\n",
       "      <td>48.34</td>\n",
       "      <td>4.40</td>\n",
       "      <td>58.46</td>\n",
       "      <td>265.42</td>\n",
       "      <td>4</td>\n",
       "      <td>8414</td>\n",
       "      <td>JO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Warners</th>\n",
       "      <td>10.62</td>\n",
       "      <td>7.74</td>\n",
       "      <td>14.48</td>\n",
       "      <td>1.97</td>\n",
       "      <td>47.97</td>\n",
       "      <td>14.01</td>\n",
       "      <td>43.73</td>\n",
       "      <td>4.90</td>\n",
       "      <td>55.39</td>\n",
       "      <td>278.05</td>\n",
       "      <td>5</td>\n",
       "      <td>8343</td>\n",
       "      <td>JO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Zsivoczky</th>\n",
       "      <td>10.91</td>\n",
       "      <td>7.14</td>\n",
       "      <td>15.31</td>\n",
       "      <td>2.12</td>\n",
       "      <td>49.40</td>\n",
       "      <td>14.95</td>\n",
       "      <td>45.62</td>\n",
       "      <td>4.70</td>\n",
       "      <td>63.45</td>\n",
       "      <td>269.54</td>\n",
       "      <td>6</td>\n",
       "      <td>8287</td>\n",
       "      <td>JO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Hernu</th>\n",
       "      <td>10.97</td>\n",
       "      <td>7.19</td>\n",
       "      <td>14.65</td>\n",
       "      <td>2.03</td>\n",
       "      <td>48.73</td>\n",
       "      <td>14.25</td>\n",
       "      <td>44.72</td>\n",
       "      <td>4.80</td>\n",
       "      <td>57.76</td>\n",
       "      <td>264.35</td>\n",
       "      <td>7</td>\n",
       "      <td>8237</td>\n",
       "      <td>JO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Nool</th>\n",
       "      <td>10.80</td>\n",
       "      <td>7.53</td>\n",
       "      <td>14.26</td>\n",
       "      <td>1.88</td>\n",
       "      <td>48.81</td>\n",
       "      <td>14.80</td>\n",
       "      <td>42.05</td>\n",
       "      <td>5.40</td>\n",
       "      <td>61.33</td>\n",
       "      <td>276.33</td>\n",
       "      <td>8</td>\n",
       "      <td>8235</td>\n",
       "      <td>JO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Bernard</th>\n",
       "      <td>10.69</td>\n",
       "      <td>7.48</td>\n",
       "      <td>14.80</td>\n",
       "      <td>2.12</td>\n",
       "      <td>49.13</td>\n",
       "      <td>14.17</td>\n",
       "      <td>44.75</td>\n",
       "      <td>4.40</td>\n",
       "      <td>55.27</td>\n",
       "      <td>276.31</td>\n",
       "      <td>9</td>\n",
       "      <td>8225</td>\n",
       "      <td>JO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Schwarzl</th>\n",
       "      <td>10.98</td>\n",
       "      <td>7.49</td>\n",
       "      <td>14.01</td>\n",
       "      <td>1.94</td>\n",
       "      <td>49.76</td>\n",
       "      <td>14.25</td>\n",
       "      <td>42.43</td>\n",
       "      <td>5.10</td>\n",
       "      <td>56.32</td>\n",
       "      <td>273.56</td>\n",
       "      <td>10</td>\n",
       "      <td>8102</td>\n",
       "      <td>JO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Pogorelov</th>\n",
       "      <td>10.95</td>\n",
       "      <td>7.31</td>\n",
       "      <td>15.10</td>\n",
       "      <td>2.06</td>\n",
       "      <td>50.79</td>\n",
       "      <td>14.21</td>\n",
       "      <td>44.60</td>\n",
       "      <td>5.00</td>\n",
       "      <td>53.45</td>\n",
       "      <td>287.63</td>\n",
       "      <td>11</td>\n",
       "      <td>8084</td>\n",
       "      <td>JO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Schoenbeck</th>\n",
       "      <td>10.90</td>\n",
       "      <td>7.30</td>\n",
       "      <td>14.77</td>\n",
       "      <td>1.88</td>\n",
       "      <td>50.30</td>\n",
       "      <td>14.34</td>\n",
       "      <td>44.41</td>\n",
       "      <td>5.00</td>\n",
       "      <td>60.89</td>\n",
       "      <td>278.82</td>\n",
       "      <td>12</td>\n",
       "      <td>8077</td>\n",
       "      <td>JO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Barras</th>\n",
       "      <td>11.14</td>\n",
       "      <td>6.99</td>\n",
       "      <td>14.91</td>\n",
       "      <td>1.94</td>\n",
       "      <td>49.41</td>\n",
       "      <td>14.37</td>\n",
       "      <td>44.83</td>\n",
       "      <td>4.60</td>\n",
       "      <td>64.55</td>\n",
       "      <td>267.09</td>\n",
       "      <td>13</td>\n",
       "      <td>8067</td>\n",
       "      <td>JO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Smith</th>\n",
       "      <td>10.85</td>\n",
       "      <td>6.81</td>\n",
       "      <td>15.24</td>\n",
       "      <td>1.91</td>\n",
       "      <td>49.27</td>\n",
       "      <td>14.01</td>\n",
       "      <td>49.02</td>\n",
       "      <td>4.20</td>\n",
       "      <td>61.52</td>\n",
       "      <td>272.74</td>\n",
       "      <td>14</td>\n",
       "      <td>8023</td>\n",
       "      <td>JO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Averyanov</th>\n",
       "      <td>10.55</td>\n",
       "      <td>7.34</td>\n",
       "      <td>14.44</td>\n",
       "      <td>1.94</td>\n",
       "      <td>49.72</td>\n",
       "      <td>14.39</td>\n",
       "      <td>39.88</td>\n",
       "      <td>4.80</td>\n",
       "      <td>54.51</td>\n",
       "      <td>271.02</td>\n",
       "      <td>15</td>\n",
       "      <td>8021</td>\n",
       "      <td>JO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Ojaniemi</th>\n",
       "      <td>10.68</td>\n",
       "      <td>7.50</td>\n",
       "      <td>14.97</td>\n",
       "      <td>1.94</td>\n",
       "      <td>49.12</td>\n",
       "      <td>15.01</td>\n",
       "      <td>40.35</td>\n",
       "      <td>4.60</td>\n",
       "      <td>59.26</td>\n",
       "      <td>275.71</td>\n",
       "      <td>16</td>\n",
       "      <td>8006</td>\n",
       "      <td>JO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Smirnov</th>\n",
       "      <td>10.89</td>\n",
       "      <td>7.07</td>\n",
       "      <td>13.88</td>\n",
       "      <td>1.94</td>\n",
       "      <td>49.11</td>\n",
       "      <td>14.77</td>\n",
       "      <td>42.47</td>\n",
       "      <td>4.70</td>\n",
       "      <td>60.88</td>\n",
       "      <td>263.31</td>\n",
       "      <td>17</td>\n",
       "      <td>7993</td>\n",
       "      <td>JO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Qi</th>\n",
       "      <td>11.06</td>\n",
       "      <td>7.34</td>\n",
       "      <td>13.55</td>\n",
       "      <td>1.97</td>\n",
       "      <td>49.65</td>\n",
       "      <td>14.78</td>\n",
       "      <td>45.13</td>\n",
       "      <td>4.50</td>\n",
       "      <td>60.79</td>\n",
       "      <td>272.63</td>\n",
       "      <td>18</td>\n",
       "      <td>7934</td>\n",
       "      <td>JO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Drews</th>\n",
       "      <td>10.87</td>\n",
       "      <td>7.38</td>\n",
       "      <td>13.07</td>\n",
       "      <td>1.88</td>\n",
       "      <td>48.51</td>\n",
       "      <td>14.01</td>\n",
       "      <td>40.11</td>\n",
       "      <td>5.00</td>\n",
       "      <td>51.53</td>\n",
       "      <td>274.21</td>\n",
       "      <td>19</td>\n",
       "      <td>7926</td>\n",
       "      <td>JO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Parkhomenko</th>\n",
       "      <td>11.14</td>\n",
       "      <td>6.61</td>\n",
       "      <td>15.69</td>\n",
       "      <td>2.03</td>\n",
       "      <td>51.04</td>\n",
       "      <td>14.88</td>\n",
       "      <td>41.90</td>\n",
       "      <td>4.80</td>\n",
       "      <td>65.82</td>\n",
       "      <td>277.94</td>\n",
       "      <td>20</td>\n",
       "      <td>7918</td>\n",
       "      <td>JO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Terek</th>\n",
       "      <td>10.92</td>\n",
       "      <td>6.94</td>\n",
       "      <td>15.15</td>\n",
       "      <td>1.94</td>\n",
       "      <td>49.56</td>\n",
       "      <td>15.12</td>\n",
       "      <td>45.62</td>\n",
       "      <td>5.30</td>\n",
       "      <td>50.62</td>\n",
       "      <td>290.36</td>\n",
       "      <td>21</td>\n",
       "      <td>7893</td>\n",
       "      <td>JO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gomez</th>\n",
       "      <td>11.08</td>\n",
       "      <td>7.26</td>\n",
       "      <td>14.57</td>\n",
       "      <td>1.85</td>\n",
       "      <td>48.61</td>\n",
       "      <td>14.41</td>\n",
       "      <td>40.95</td>\n",
       "      <td>4.40</td>\n",
       "      <td>60.71</td>\n",
       "      <td>269.70</td>\n",
       "      <td>22</td>\n",
       "      <td>7865</td>\n",
       "      <td>JO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Turi</th>\n",
       "      <td>11.08</td>\n",
       "      <td>6.91</td>\n",
       "      <td>13.62</td>\n",
       "      <td>2.03</td>\n",
       "      <td>51.67</td>\n",
       "      <td>14.26</td>\n",
       "      <td>39.83</td>\n",
       "      <td>4.80</td>\n",
       "      <td>59.34</td>\n",
       "      <td>290.01</td>\n",
       "      <td>23</td>\n",
       "      <td>7708</td>\n",
       "      <td>JO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Lorenzo</th>\n",
       "      <td>11.10</td>\n",
       "      <td>7.03</td>\n",
       "      <td>13.22</td>\n",
       "      <td>1.85</td>\n",
       "      <td>49.34</td>\n",
       "      <td>15.38</td>\n",
       "      <td>40.22</td>\n",
       "      <td>4.50</td>\n",
       "      <td>58.36</td>\n",
       "      <td>263.08</td>\n",
       "      <td>24</td>\n",
       "      <td>7592</td>\n",
       "      <td>JO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Karlivans</th>\n",
       "      <td>11.33</td>\n",
       "      <td>7.26</td>\n",
       "      <td>13.30</td>\n",
       "      <td>1.97</td>\n",
       "      <td>50.54</td>\n",
       "      <td>14.98</td>\n",
       "      <td>43.34</td>\n",
       "      <td>4.50</td>\n",
       "      <td>52.92</td>\n",
       "      <td>278.67</td>\n",
       "      <td>25</td>\n",
       "      <td>7583</td>\n",
       "      <td>JO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Korkizoglou</th>\n",
       "      <td>10.86</td>\n",
       "      <td>7.07</td>\n",
       "      <td>14.81</td>\n",
       "      <td>1.94</td>\n",
       "      <td>51.16</td>\n",
       "      <td>14.96</td>\n",
       "      <td>46.07</td>\n",
       "      <td>4.70</td>\n",
       "      <td>53.05</td>\n",
       "      <td>317.00</td>\n",
       "      <td>26</td>\n",
       "      <td>7573</td>\n",
       "      <td>JO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Uldal</th>\n",
       "      <td>11.23</td>\n",
       "      <td>6.99</td>\n",
       "      <td>13.53</td>\n",
       "      <td>1.85</td>\n",
       "      <td>50.95</td>\n",
       "      <td>15.09</td>\n",
       "      <td>43.01</td>\n",
       "      <td>4.50</td>\n",
       "      <td>60.00</td>\n",
       "      <td>281.70</td>\n",
       "      <td>27</td>\n",
       "      <td>7495</td>\n",
       "      <td>JO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Casarsa</th>\n",
       "      <td>11.36</td>\n",
       "      <td>6.68</td>\n",
       "      <td>14.92</td>\n",
       "      <td>1.94</td>\n",
       "      <td>53.20</td>\n",
       "      <td>15.39</td>\n",
       "      <td>48.66</td>\n",
       "      <td>4.40</td>\n",
       "      <td>58.62</td>\n",
       "      <td>296.12</td>\n",
       "      <td>28</td>\n",
       "      <td>7404</td>\n",
       "      <td>JO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SEBRLE</th>\n",
       "      <td>11.04</td>\n",
       "      <td>7.58</td>\n",
       "      <td>14.83</td>\n",
       "      <td>2.07</td>\n",
       "      <td>49.81</td>\n",
       "      <td>14.69</td>\n",
       "      <td>43.75</td>\n",
       "      <td>5.02</td>\n",
       "      <td>63.19</td>\n",
       "      <td>291.70</td>\n",
       "      <td>1</td>\n",
       "      <td>8217</td>\n",
       "      <td>Decastar</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CLAY</th>\n",
       "      <td>10.76</td>\n",
       "      <td>7.40</td>\n",
       "      <td>14.26</td>\n",
       "      <td>1.86</td>\n",
       "      <td>49.37</td>\n",
       "      <td>14.05</td>\n",
       "      <td>50.72</td>\n",
       "      <td>4.92</td>\n",
       "      <td>60.15</td>\n",
       "      <td>301.50</td>\n",
       "      <td>2</td>\n",
       "      <td>8122</td>\n",
       "      <td>Decastar</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>KARPOV</th>\n",
       "      <td>11.02</td>\n",
       "      <td>7.30</td>\n",
       "      <td>14.77</td>\n",
       "      <td>2.04</td>\n",
       "      <td>48.37</td>\n",
       "      <td>14.09</td>\n",
       "      <td>48.95</td>\n",
       "      <td>4.92</td>\n",
       "      <td>50.31</td>\n",
       "      <td>300.20</td>\n",
       "      <td>3</td>\n",
       "      <td>8099</td>\n",
       "      <td>Decastar</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BERNARD</th>\n",
       "      <td>11.02</td>\n",
       "      <td>7.23</td>\n",
       "      <td>14.25</td>\n",
       "      <td>1.92</td>\n",
       "      <td>48.93</td>\n",
       "      <td>14.99</td>\n",
       "      <td>40.87</td>\n",
       "      <td>5.32</td>\n",
       "      <td>62.77</td>\n",
       "      <td>280.10</td>\n",
       "      <td>4</td>\n",
       "      <td>8067</td>\n",
       "      <td>Decastar</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>YURKOV</th>\n",
       "      <td>11.34</td>\n",
       "      <td>7.09</td>\n",
       "      <td>15.19</td>\n",
       "      <td>2.10</td>\n",
       "      <td>50.42</td>\n",
       "      <td>15.31</td>\n",
       "      <td>46.26</td>\n",
       "      <td>4.72</td>\n",
       "      <td>63.44</td>\n",
       "      <td>276.40</td>\n",
       "      <td>5</td>\n",
       "      <td>8036</td>\n",
       "      <td>Decastar</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>WARNERS</th>\n",
       "      <td>11.11</td>\n",
       "      <td>7.60</td>\n",
       "      <td>14.31</td>\n",
       "      <td>1.98</td>\n",
       "      <td>48.68</td>\n",
       "      <td>14.23</td>\n",
       "      <td>41.10</td>\n",
       "      <td>4.92</td>\n",
       "      <td>51.77</td>\n",
       "      <td>278.10</td>\n",
       "      <td>6</td>\n",
       "      <td>8030</td>\n",
       "      <td>Decastar</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ZSIVOCZKY</th>\n",
       "      <td>11.13</td>\n",
       "      <td>7.30</td>\n",
       "      <td>13.48</td>\n",
       "      <td>2.01</td>\n",
       "      <td>48.62</td>\n",
       "      <td>14.17</td>\n",
       "      <td>45.67</td>\n",
       "      <td>4.42</td>\n",
       "      <td>55.37</td>\n",
       "      <td>268.00</td>\n",
       "      <td>7</td>\n",
       "      <td>8004</td>\n",
       "      <td>Decastar</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>McMULLEN</th>\n",
       "      <td>10.83</td>\n",
       "      <td>7.31</td>\n",
       "      <td>13.76</td>\n",
       "      <td>2.13</td>\n",
       "      <td>49.91</td>\n",
       "      <td>14.38</td>\n",
       "      <td>44.41</td>\n",
       "      <td>4.42</td>\n",
       "      <td>56.37</td>\n",
       "      <td>285.10</td>\n",
       "      <td>8</td>\n",
       "      <td>7995</td>\n",
       "      <td>Decastar</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MARTINEAU</th>\n",
       "      <td>11.64</td>\n",
       "      <td>6.81</td>\n",
       "      <td>14.57</td>\n",
       "      <td>1.95</td>\n",
       "      <td>50.14</td>\n",
       "      <td>14.93</td>\n",
       "      <td>47.60</td>\n",
       "      <td>4.92</td>\n",
       "      <td>52.33</td>\n",
       "      <td>262.10</td>\n",
       "      <td>9</td>\n",
       "      <td>7802</td>\n",
       "      <td>Decastar</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>HERNU</th>\n",
       "      <td>11.37</td>\n",
       "      <td>7.56</td>\n",
       "      <td>14.41</td>\n",
       "      <td>1.86</td>\n",
       "      <td>51.10</td>\n",
       "      <td>15.06</td>\n",
       "      <td>44.99</td>\n",
       "      <td>4.82</td>\n",
       "      <td>57.19</td>\n",
       "      <td>285.10</td>\n",
       "      <td>10</td>\n",
       "      <td>7733</td>\n",
       "      <td>Decastar</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BARRAS</th>\n",
       "      <td>11.33</td>\n",
       "      <td>6.97</td>\n",
       "      <td>14.09</td>\n",
       "      <td>1.95</td>\n",
       "      <td>49.48</td>\n",
       "      <td>14.48</td>\n",
       "      <td>42.10</td>\n",
       "      <td>4.72</td>\n",
       "      <td>55.40</td>\n",
       "      <td>282.00</td>\n",
       "      <td>11</td>\n",
       "      <td>7708</td>\n",
       "      <td>Decastar</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NOOL</th>\n",
       "      <td>11.33</td>\n",
       "      <td>7.27</td>\n",
       "      <td>12.68</td>\n",
       "      <td>1.98</td>\n",
       "      <td>49.20</td>\n",
       "      <td>15.29</td>\n",
       "      <td>37.92</td>\n",
       "      <td>4.62</td>\n",
       "      <td>57.44</td>\n",
       "      <td>266.60</td>\n",
       "      <td>12</td>\n",
       "      <td>7651</td>\n",
       "      <td>Decastar</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BOURGUIGNON</th>\n",
       "      <td>11.36</td>\n",
       "      <td>6.80</td>\n",
       "      <td>13.46</td>\n",
       "      <td>1.86</td>\n",
       "      <td>51.16</td>\n",
       "      <td>15.67</td>\n",
       "      <td>40.49</td>\n",
       "      <td>5.02</td>\n",
       "      <td>54.68</td>\n",
       "      <td>291.70</td>\n",
       "      <td>13</td>\n",
       "      <td>7313</td>\n",
       "      <td>Decastar</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              100m  Longueur  Poids  Hauteur   400m  110m H  Disque  Perche  \\\n",
       "Sebrle       10.85      7.84  16.36     2.12  48.36   14.05   48.72    5.00   \n",
       "Clay         10.44      7.96  15.23     2.06  49.19   14.13   50.11    4.90   \n",
       "Karpov       10.50      7.81  15.93     2.09  46.81   13.97   51.65    4.60   \n",
       "Macey        10.89      7.47  15.73     2.15  48.97   14.56   48.34    4.40   \n",
       "Warners      10.62      7.74  14.48     1.97  47.97   14.01   43.73    4.90   \n",
       "Zsivoczky    10.91      7.14  15.31     2.12  49.40   14.95   45.62    4.70   \n",
       "Hernu        10.97      7.19  14.65     2.03  48.73   14.25   44.72    4.80   \n",
       "Nool         10.80      7.53  14.26     1.88  48.81   14.80   42.05    5.40   \n",
       "Bernard      10.69      7.48  14.80     2.12  49.13   14.17   44.75    4.40   \n",
       "Schwarzl     10.98      7.49  14.01     1.94  49.76   14.25   42.43    5.10   \n",
       "Pogorelov    10.95      7.31  15.10     2.06  50.79   14.21   44.60    5.00   \n",
       "Schoenbeck   10.90      7.30  14.77     1.88  50.30   14.34   44.41    5.00   \n",
       "Barras       11.14      6.99  14.91     1.94  49.41   14.37   44.83    4.60   \n",
       "Smith        10.85      6.81  15.24     1.91  49.27   14.01   49.02    4.20   \n",
       "Averyanov    10.55      7.34  14.44     1.94  49.72   14.39   39.88    4.80   \n",
       "Ojaniemi     10.68      7.50  14.97     1.94  49.12   15.01   40.35    4.60   \n",
       "Smirnov      10.89      7.07  13.88     1.94  49.11   14.77   42.47    4.70   \n",
       "Qi           11.06      7.34  13.55     1.97  49.65   14.78   45.13    4.50   \n",
       "Drews        10.87      7.38  13.07     1.88  48.51   14.01   40.11    5.00   \n",
       "Parkhomenko  11.14      6.61  15.69     2.03  51.04   14.88   41.90    4.80   \n",
       "Terek        10.92      6.94  15.15     1.94  49.56   15.12   45.62    5.30   \n",
       "Gomez        11.08      7.26  14.57     1.85  48.61   14.41   40.95    4.40   \n",
       "Turi         11.08      6.91  13.62     2.03  51.67   14.26   39.83    4.80   \n",
       "Lorenzo      11.10      7.03  13.22     1.85  49.34   15.38   40.22    4.50   \n",
       "Karlivans    11.33      7.26  13.30     1.97  50.54   14.98   43.34    4.50   \n",
       "Korkizoglou  10.86      7.07  14.81     1.94  51.16   14.96   46.07    4.70   \n",
       "Uldal        11.23      6.99  13.53     1.85  50.95   15.09   43.01    4.50   \n",
       "Casarsa      11.36      6.68  14.92     1.94  53.20   15.39   48.66    4.40   \n",
       "SEBRLE       11.04      7.58  14.83     2.07  49.81   14.69   43.75    5.02   \n",
       "CLAY         10.76      7.40  14.26     1.86  49.37   14.05   50.72    4.92   \n",
       "KARPOV       11.02      7.30  14.77     2.04  48.37   14.09   48.95    4.92   \n",
       "BERNARD      11.02      7.23  14.25     1.92  48.93   14.99   40.87    5.32   \n",
       "YURKOV       11.34      7.09  15.19     2.10  50.42   15.31   46.26    4.72   \n",
       "WARNERS      11.11      7.60  14.31     1.98  48.68   14.23   41.10    4.92   \n",
       "ZSIVOCZKY    11.13      7.30  13.48     2.01  48.62   14.17   45.67    4.42   \n",
       "McMULLEN     10.83      7.31  13.76     2.13  49.91   14.38   44.41    4.42   \n",
       "MARTINEAU    11.64      6.81  14.57     1.95  50.14   14.93   47.60    4.92   \n",
       "HERNU        11.37      7.56  14.41     1.86  51.10   15.06   44.99    4.82   \n",
       "BARRAS       11.33      6.97  14.09     1.95  49.48   14.48   42.10    4.72   \n",
       "NOOL         11.33      7.27  12.68     1.98  49.20   15.29   37.92    4.62   \n",
       "BOURGUIGNON  11.36      6.80  13.46     1.86  51.16   15.67   40.49    5.02   \n",
       "\n",
       "             Javelot   1500m  Classement  Points Competition  \n",
       "Sebrle         70.52  280.01           1    8893          JO  \n",
       "Clay           69.71  282.00           2    8820          JO  \n",
       "Karpov         55.54  278.11           3    8725          JO  \n",
       "Macey          58.46  265.42           4    8414          JO  \n",
       "Warners        55.39  278.05           5    8343          JO  \n",
       "Zsivoczky      63.45  269.54           6    8287          JO  \n",
       "Hernu          57.76  264.35           7    8237          JO  \n",
       "Nool           61.33  276.33           8    8235          JO  \n",
       "Bernard        55.27  276.31           9    8225          JO  \n",
       "Schwarzl       56.32  273.56          10    8102          JO  \n",
       "Pogorelov      53.45  287.63          11    8084          JO  \n",
       "Schoenbeck     60.89  278.82          12    8077          JO  \n",
       "Barras         64.55  267.09          13    8067          JO  \n",
       "Smith          61.52  272.74          14    8023          JO  \n",
       "Averyanov      54.51  271.02          15    8021          JO  \n",
       "Ojaniemi       59.26  275.71          16    8006          JO  \n",
       "Smirnov        60.88  263.31          17    7993          JO  \n",
       "Qi             60.79  272.63          18    7934          JO  \n",
       "Drews          51.53  274.21          19    7926          JO  \n",
       "Parkhomenko    65.82  277.94          20    7918          JO  \n",
       "Terek          50.62  290.36          21    7893          JO  \n",
       "Gomez          60.71  269.70          22    7865          JO  \n",
       "Turi           59.34  290.01          23    7708          JO  \n",
       "Lorenzo        58.36  263.08          24    7592          JO  \n",
       "Karlivans      52.92  278.67          25    7583          JO  \n",
       "Korkizoglou    53.05  317.00          26    7573          JO  \n",
       "Uldal          60.00  281.70          27    7495          JO  \n",
       "Casarsa        58.62  296.12          28    7404          JO  \n",
       "SEBRLE         63.19  291.70           1    8217    Decastar  \n",
       "CLAY           60.15  301.50           2    8122    Decastar  \n",
       "KARPOV         50.31  300.20           3    8099    Decastar  \n",
       "BERNARD        62.77  280.10           4    8067    Decastar  \n",
       "YURKOV         63.44  276.40           5    8036    Decastar  \n",
       "WARNERS        51.77  278.10           6    8030    Decastar  \n",
       "ZSIVOCZKY      55.37  268.00           7    8004    Decastar  \n",
       "McMULLEN       56.37  285.10           8    7995    Decastar  \n",
       "MARTINEAU      52.33  262.10           9    7802    Decastar  \n",
       "HERNU          57.19  285.10          10    7733    Decastar  \n",
       "BARRAS         55.40  282.00          11    7708    Decastar  \n",
       "NOOL           57.44  266.60          12    7651    Decastar  \n",
       "BOURGUIGNON    54.68  291.70          13    7313    Decastar  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np \n",
    "decathlon = pd.read_csv(\"decathlon.csv\", sep=\";\",\n",
    "index_col = 0)\n",
    "print(f\"Dimensions = {decathlon.shape}\")\n",
    "decathlon\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(41, 13)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decathlon.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>100m</th>\n",
       "      <th>Longueur</th>\n",
       "      <th>Poids</th>\n",
       "      <th>Hauteur</th>\n",
       "      <th>400m</th>\n",
       "      <th>110m H</th>\n",
       "      <th>Disque</th>\n",
       "      <th>Perche</th>\n",
       "      <th>Javelot</th>\n",
       "      <th>1500m</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Sebrle</th>\n",
       "      <td>10.85</td>\n",
       "      <td>7.84</td>\n",
       "      <td>16.36</td>\n",
       "      <td>2.12</td>\n",
       "      <td>48.36</td>\n",
       "      <td>14.05</td>\n",
       "      <td>48.72</td>\n",
       "      <td>5.00</td>\n",
       "      <td>70.52</td>\n",
       "      <td>280.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Clay</th>\n",
       "      <td>10.44</td>\n",
       "      <td>7.96</td>\n",
       "      <td>15.23</td>\n",
       "      <td>2.06</td>\n",
       "      <td>49.19</td>\n",
       "      <td>14.13</td>\n",
       "      <td>50.11</td>\n",
       "      <td>4.90</td>\n",
       "      <td>69.71</td>\n",
       "      <td>282.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Karpov</th>\n",
       "      <td>10.50</td>\n",
       "      <td>7.81</td>\n",
       "      <td>15.93</td>\n",
       "      <td>2.09</td>\n",
       "      <td>46.81</td>\n",
       "      <td>13.97</td>\n",
       "      <td>51.65</td>\n",
       "      <td>4.60</td>\n",
       "      <td>55.54</td>\n",
       "      <td>278.11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Macey</th>\n",
       "      <td>10.89</td>\n",
       "      <td>7.47</td>\n",
       "      <td>15.73</td>\n",
       "      <td>2.15</td>\n",
       "      <td>48.97</td>\n",
       "      <td>14.56</td>\n",
       "      <td>48.34</td>\n",
       "      <td>4.40</td>\n",
       "      <td>58.46</td>\n",
       "      <td>265.42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Warners</th>\n",
       "      <td>10.62</td>\n",
       "      <td>7.74</td>\n",
       "      <td>14.48</td>\n",
       "      <td>1.97</td>\n",
       "      <td>47.97</td>\n",
       "      <td>14.01</td>\n",
       "      <td>43.73</td>\n",
       "      <td>4.90</td>\n",
       "      <td>55.39</td>\n",
       "      <td>278.05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Zsivoczky</th>\n",
       "      <td>10.91</td>\n",
       "      <td>7.14</td>\n",
       "      <td>15.31</td>\n",
       "      <td>2.12</td>\n",
       "      <td>49.40</td>\n",
       "      <td>14.95</td>\n",
       "      <td>45.62</td>\n",
       "      <td>4.70</td>\n",
       "      <td>63.45</td>\n",
       "      <td>269.54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Hernu</th>\n",
       "      <td>10.97</td>\n",
       "      <td>7.19</td>\n",
       "      <td>14.65</td>\n",
       "      <td>2.03</td>\n",
       "      <td>48.73</td>\n",
       "      <td>14.25</td>\n",
       "      <td>44.72</td>\n",
       "      <td>4.80</td>\n",
       "      <td>57.76</td>\n",
       "      <td>264.35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Nool</th>\n",
       "      <td>10.80</td>\n",
       "      <td>7.53</td>\n",
       "      <td>14.26</td>\n",
       "      <td>1.88</td>\n",
       "      <td>48.81</td>\n",
       "      <td>14.80</td>\n",
       "      <td>42.05</td>\n",
       "      <td>5.40</td>\n",
       "      <td>61.33</td>\n",
       "      <td>276.33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Bernard</th>\n",
       "      <td>10.69</td>\n",
       "      <td>7.48</td>\n",
       "      <td>14.80</td>\n",
       "      <td>2.12</td>\n",
       "      <td>49.13</td>\n",
       "      <td>14.17</td>\n",
       "      <td>44.75</td>\n",
       "      <td>4.40</td>\n",
       "      <td>55.27</td>\n",
       "      <td>276.31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Schwarzl</th>\n",
       "      <td>10.98</td>\n",
       "      <td>7.49</td>\n",
       "      <td>14.01</td>\n",
       "      <td>1.94</td>\n",
       "      <td>49.76</td>\n",
       "      <td>14.25</td>\n",
       "      <td>42.43</td>\n",
       "      <td>5.10</td>\n",
       "      <td>56.32</td>\n",
       "      <td>273.56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Pogorelov</th>\n",
       "      <td>10.95</td>\n",
       "      <td>7.31</td>\n",
       "      <td>15.10</td>\n",
       "      <td>2.06</td>\n",
       "      <td>50.79</td>\n",
       "      <td>14.21</td>\n",
       "      <td>44.60</td>\n",
       "      <td>5.00</td>\n",
       "      <td>53.45</td>\n",
       "      <td>287.63</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Schoenbeck</th>\n",
       "      <td>10.90</td>\n",
       "      <td>7.30</td>\n",
       "      <td>14.77</td>\n",
       "      <td>1.88</td>\n",
       "      <td>50.30</td>\n",
       "      <td>14.34</td>\n",
       "      <td>44.41</td>\n",
       "      <td>5.00</td>\n",
       "      <td>60.89</td>\n",
       "      <td>278.82</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Barras</th>\n",
       "      <td>11.14</td>\n",
       "      <td>6.99</td>\n",
       "      <td>14.91</td>\n",
       "      <td>1.94</td>\n",
       "      <td>49.41</td>\n",
       "      <td>14.37</td>\n",
       "      <td>44.83</td>\n",
       "      <td>4.60</td>\n",
       "      <td>64.55</td>\n",
       "      <td>267.09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Smith</th>\n",
       "      <td>10.85</td>\n",
       "      <td>6.81</td>\n",
       "      <td>15.24</td>\n",
       "      <td>1.91</td>\n",
       "      <td>49.27</td>\n",
       "      <td>14.01</td>\n",
       "      <td>49.02</td>\n",
       "      <td>4.20</td>\n",
       "      <td>61.52</td>\n",
       "      <td>272.74</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Averyanov</th>\n",
       "      <td>10.55</td>\n",
       "      <td>7.34</td>\n",
       "      <td>14.44</td>\n",
       "      <td>1.94</td>\n",
       "      <td>49.72</td>\n",
       "      <td>14.39</td>\n",
       "      <td>39.88</td>\n",
       "      <td>4.80</td>\n",
       "      <td>54.51</td>\n",
       "      <td>271.02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Ojaniemi</th>\n",
       "      <td>10.68</td>\n",
       "      <td>7.50</td>\n",
       "      <td>14.97</td>\n",
       "      <td>1.94</td>\n",
       "      <td>49.12</td>\n",
       "      <td>15.01</td>\n",
       "      <td>40.35</td>\n",
       "      <td>4.60</td>\n",
       "      <td>59.26</td>\n",
       "      <td>275.71</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Smirnov</th>\n",
       "      <td>10.89</td>\n",
       "      <td>7.07</td>\n",
       "      <td>13.88</td>\n",
       "      <td>1.94</td>\n",
       "      <td>49.11</td>\n",
       "      <td>14.77</td>\n",
       "      <td>42.47</td>\n",
       "      <td>4.70</td>\n",
       "      <td>60.88</td>\n",
       "      <td>263.31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Qi</th>\n",
       "      <td>11.06</td>\n",
       "      <td>7.34</td>\n",
       "      <td>13.55</td>\n",
       "      <td>1.97</td>\n",
       "      <td>49.65</td>\n",
       "      <td>14.78</td>\n",
       "      <td>45.13</td>\n",
       "      <td>4.50</td>\n",
       "      <td>60.79</td>\n",
       "      <td>272.63</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Drews</th>\n",
       "      <td>10.87</td>\n",
       "      <td>7.38</td>\n",
       "      <td>13.07</td>\n",
       "      <td>1.88</td>\n",
       "      <td>48.51</td>\n",
       "      <td>14.01</td>\n",
       "      <td>40.11</td>\n",
       "      <td>5.00</td>\n",
       "      <td>51.53</td>\n",
       "      <td>274.21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Parkhomenko</th>\n",
       "      <td>11.14</td>\n",
       "      <td>6.61</td>\n",
       "      <td>15.69</td>\n",
       "      <td>2.03</td>\n",
       "      <td>51.04</td>\n",
       "      <td>14.88</td>\n",
       "      <td>41.90</td>\n",
       "      <td>4.80</td>\n",
       "      <td>65.82</td>\n",
       "      <td>277.94</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Terek</th>\n",
       "      <td>10.92</td>\n",
       "      <td>6.94</td>\n",
       "      <td>15.15</td>\n",
       "      <td>1.94</td>\n",
       "      <td>49.56</td>\n",
       "      <td>15.12</td>\n",
       "      <td>45.62</td>\n",
       "      <td>5.30</td>\n",
       "      <td>50.62</td>\n",
       "      <td>290.36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gomez</th>\n",
       "      <td>11.08</td>\n",
       "      <td>7.26</td>\n",
       "      <td>14.57</td>\n",
       "      <td>1.85</td>\n",
       "      <td>48.61</td>\n",
       "      <td>14.41</td>\n",
       "      <td>40.95</td>\n",
       "      <td>4.40</td>\n",
       "      <td>60.71</td>\n",
       "      <td>269.70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Turi</th>\n",
       "      <td>11.08</td>\n",
       "      <td>6.91</td>\n",
       "      <td>13.62</td>\n",
       "      <td>2.03</td>\n",
       "      <td>51.67</td>\n",
       "      <td>14.26</td>\n",
       "      <td>39.83</td>\n",
       "      <td>4.80</td>\n",
       "      <td>59.34</td>\n",
       "      <td>290.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Lorenzo</th>\n",
       "      <td>11.10</td>\n",
       "      <td>7.03</td>\n",
       "      <td>13.22</td>\n",
       "      <td>1.85</td>\n",
       "      <td>49.34</td>\n",
       "      <td>15.38</td>\n",
       "      <td>40.22</td>\n",
       "      <td>4.50</td>\n",
       "      <td>58.36</td>\n",
       "      <td>263.08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Karlivans</th>\n",
       "      <td>11.33</td>\n",
       "      <td>7.26</td>\n",
       "      <td>13.30</td>\n",
       "      <td>1.97</td>\n",
       "      <td>50.54</td>\n",
       "      <td>14.98</td>\n",
       "      <td>43.34</td>\n",
       "      <td>4.50</td>\n",
       "      <td>52.92</td>\n",
       "      <td>278.67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Korkizoglou</th>\n",
       "      <td>10.86</td>\n",
       "      <td>7.07</td>\n",
       "      <td>14.81</td>\n",
       "      <td>1.94</td>\n",
       "      <td>51.16</td>\n",
       "      <td>14.96</td>\n",
       "      <td>46.07</td>\n",
       "      <td>4.70</td>\n",
       "      <td>53.05</td>\n",
       "      <td>317.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Uldal</th>\n",
       "      <td>11.23</td>\n",
       "      <td>6.99</td>\n",
       "      <td>13.53</td>\n",
       "      <td>1.85</td>\n",
       "      <td>50.95</td>\n",
       "      <td>15.09</td>\n",
       "      <td>43.01</td>\n",
       "      <td>4.50</td>\n",
       "      <td>60.00</td>\n",
       "      <td>281.70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Casarsa</th>\n",
       "      <td>11.36</td>\n",
       "      <td>6.68</td>\n",
       "      <td>14.92</td>\n",
       "      <td>1.94</td>\n",
       "      <td>53.20</td>\n",
       "      <td>15.39</td>\n",
       "      <td>48.66</td>\n",
       "      <td>4.40</td>\n",
       "      <td>58.62</td>\n",
       "      <td>296.12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SEBRLE</th>\n",
       "      <td>11.04</td>\n",
       "      <td>7.58</td>\n",
       "      <td>14.83</td>\n",
       "      <td>2.07</td>\n",
       "      <td>49.81</td>\n",
       "      <td>14.69</td>\n",
       "      <td>43.75</td>\n",
       "      <td>5.02</td>\n",
       "      <td>63.19</td>\n",
       "      <td>291.70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CLAY</th>\n",
       "      <td>10.76</td>\n",
       "      <td>7.40</td>\n",
       "      <td>14.26</td>\n",
       "      <td>1.86</td>\n",
       "      <td>49.37</td>\n",
       "      <td>14.05</td>\n",
       "      <td>50.72</td>\n",
       "      <td>4.92</td>\n",
       "      <td>60.15</td>\n",
       "      <td>301.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>KARPOV</th>\n",
       "      <td>11.02</td>\n",
       "      <td>7.30</td>\n",
       "      <td>14.77</td>\n",
       "      <td>2.04</td>\n",
       "      <td>48.37</td>\n",
       "      <td>14.09</td>\n",
       "      <td>48.95</td>\n",
       "      <td>4.92</td>\n",
       "      <td>50.31</td>\n",
       "      <td>300.20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BERNARD</th>\n",
       "      <td>11.02</td>\n",
       "      <td>7.23</td>\n",
       "      <td>14.25</td>\n",
       "      <td>1.92</td>\n",
       "      <td>48.93</td>\n",
       "      <td>14.99</td>\n",
       "      <td>40.87</td>\n",
       "      <td>5.32</td>\n",
       "      <td>62.77</td>\n",
       "      <td>280.10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>YURKOV</th>\n",
       "      <td>11.34</td>\n",
       "      <td>7.09</td>\n",
       "      <td>15.19</td>\n",
       "      <td>2.10</td>\n",
       "      <td>50.42</td>\n",
       "      <td>15.31</td>\n",
       "      <td>46.26</td>\n",
       "      <td>4.72</td>\n",
       "      <td>63.44</td>\n",
       "      <td>276.40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>WARNERS</th>\n",
       "      <td>11.11</td>\n",
       "      <td>7.60</td>\n",
       "      <td>14.31</td>\n",
       "      <td>1.98</td>\n",
       "      <td>48.68</td>\n",
       "      <td>14.23</td>\n",
       "      <td>41.10</td>\n",
       "      <td>4.92</td>\n",
       "      <td>51.77</td>\n",
       "      <td>278.10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ZSIVOCZKY</th>\n",
       "      <td>11.13</td>\n",
       "      <td>7.30</td>\n",
       "      <td>13.48</td>\n",
       "      <td>2.01</td>\n",
       "      <td>48.62</td>\n",
       "      <td>14.17</td>\n",
       "      <td>45.67</td>\n",
       "      <td>4.42</td>\n",
       "      <td>55.37</td>\n",
       "      <td>268.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>McMULLEN</th>\n",
       "      <td>10.83</td>\n",
       "      <td>7.31</td>\n",
       "      <td>13.76</td>\n",
       "      <td>2.13</td>\n",
       "      <td>49.91</td>\n",
       "      <td>14.38</td>\n",
       "      <td>44.41</td>\n",
       "      <td>4.42</td>\n",
       "      <td>56.37</td>\n",
       "      <td>285.10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MARTINEAU</th>\n",
       "      <td>11.64</td>\n",
       "      <td>6.81</td>\n",
       "      <td>14.57</td>\n",
       "      <td>1.95</td>\n",
       "      <td>50.14</td>\n",
       "      <td>14.93</td>\n",
       "      <td>47.60</td>\n",
       "      <td>4.92</td>\n",
       "      <td>52.33</td>\n",
       "      <td>262.10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>HERNU</th>\n",
       "      <td>11.37</td>\n",
       "      <td>7.56</td>\n",
       "      <td>14.41</td>\n",
       "      <td>1.86</td>\n",
       "      <td>51.10</td>\n",
       "      <td>15.06</td>\n",
       "      <td>44.99</td>\n",
       "      <td>4.82</td>\n",
       "      <td>57.19</td>\n",
       "      <td>285.10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BARRAS</th>\n",
       "      <td>11.33</td>\n",
       "      <td>6.97</td>\n",
       "      <td>14.09</td>\n",
       "      <td>1.95</td>\n",
       "      <td>49.48</td>\n",
       "      <td>14.48</td>\n",
       "      <td>42.10</td>\n",
       "      <td>4.72</td>\n",
       "      <td>55.40</td>\n",
       "      <td>282.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NOOL</th>\n",
       "      <td>11.33</td>\n",
       "      <td>7.27</td>\n",
       "      <td>12.68</td>\n",
       "      <td>1.98</td>\n",
       "      <td>49.20</td>\n",
       "      <td>15.29</td>\n",
       "      <td>37.92</td>\n",
       "      <td>4.62</td>\n",
       "      <td>57.44</td>\n",
       "      <td>266.60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BOURGUIGNON</th>\n",
       "      <td>11.36</td>\n",
       "      <td>6.80</td>\n",
       "      <td>13.46</td>\n",
       "      <td>1.86</td>\n",
       "      <td>51.16</td>\n",
       "      <td>15.67</td>\n",
       "      <td>40.49</td>\n",
       "      <td>5.02</td>\n",
       "      <td>54.68</td>\n",
       "      <td>291.70</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              100m  Longueur  Poids  Hauteur   400m  110m H  Disque  Perche  \\\n",
       "Sebrle       10.85      7.84  16.36     2.12  48.36   14.05   48.72    5.00   \n",
       "Clay         10.44      7.96  15.23     2.06  49.19   14.13   50.11    4.90   \n",
       "Karpov       10.50      7.81  15.93     2.09  46.81   13.97   51.65    4.60   \n",
       "Macey        10.89      7.47  15.73     2.15  48.97   14.56   48.34    4.40   \n",
       "Warners      10.62      7.74  14.48     1.97  47.97   14.01   43.73    4.90   \n",
       "Zsivoczky    10.91      7.14  15.31     2.12  49.40   14.95   45.62    4.70   \n",
       "Hernu        10.97      7.19  14.65     2.03  48.73   14.25   44.72    4.80   \n",
       "Nool         10.80      7.53  14.26     1.88  48.81   14.80   42.05    5.40   \n",
       "Bernard      10.69      7.48  14.80     2.12  49.13   14.17   44.75    4.40   \n",
       "Schwarzl     10.98      7.49  14.01     1.94  49.76   14.25   42.43    5.10   \n",
       "Pogorelov    10.95      7.31  15.10     2.06  50.79   14.21   44.60    5.00   \n",
       "Schoenbeck   10.90      7.30  14.77     1.88  50.30   14.34   44.41    5.00   \n",
       "Barras       11.14      6.99  14.91     1.94  49.41   14.37   44.83    4.60   \n",
       "Smith        10.85      6.81  15.24     1.91  49.27   14.01   49.02    4.20   \n",
       "Averyanov    10.55      7.34  14.44     1.94  49.72   14.39   39.88    4.80   \n",
       "Ojaniemi     10.68      7.50  14.97     1.94  49.12   15.01   40.35    4.60   \n",
       "Smirnov      10.89      7.07  13.88     1.94  49.11   14.77   42.47    4.70   \n",
       "Qi           11.06      7.34  13.55     1.97  49.65   14.78   45.13    4.50   \n",
       "Drews        10.87      7.38  13.07     1.88  48.51   14.01   40.11    5.00   \n",
       "Parkhomenko  11.14      6.61  15.69     2.03  51.04   14.88   41.90    4.80   \n",
       "Terek        10.92      6.94  15.15     1.94  49.56   15.12   45.62    5.30   \n",
       "Gomez        11.08      7.26  14.57     1.85  48.61   14.41   40.95    4.40   \n",
       "Turi         11.08      6.91  13.62     2.03  51.67   14.26   39.83    4.80   \n",
       "Lorenzo      11.10      7.03  13.22     1.85  49.34   15.38   40.22    4.50   \n",
       "Karlivans    11.33      7.26  13.30     1.97  50.54   14.98   43.34    4.50   \n",
       "Korkizoglou  10.86      7.07  14.81     1.94  51.16   14.96   46.07    4.70   \n",
       "Uldal        11.23      6.99  13.53     1.85  50.95   15.09   43.01    4.50   \n",
       "Casarsa      11.36      6.68  14.92     1.94  53.20   15.39   48.66    4.40   \n",
       "SEBRLE       11.04      7.58  14.83     2.07  49.81   14.69   43.75    5.02   \n",
       "CLAY         10.76      7.40  14.26     1.86  49.37   14.05   50.72    4.92   \n",
       "KARPOV       11.02      7.30  14.77     2.04  48.37   14.09   48.95    4.92   \n",
       "BERNARD      11.02      7.23  14.25     1.92  48.93   14.99   40.87    5.32   \n",
       "YURKOV       11.34      7.09  15.19     2.10  50.42   15.31   46.26    4.72   \n",
       "WARNERS      11.11      7.60  14.31     1.98  48.68   14.23   41.10    4.92   \n",
       "ZSIVOCZKY    11.13      7.30  13.48     2.01  48.62   14.17   45.67    4.42   \n",
       "McMULLEN     10.83      7.31  13.76     2.13  49.91   14.38   44.41    4.42   \n",
       "MARTINEAU    11.64      6.81  14.57     1.95  50.14   14.93   47.60    4.92   \n",
       "HERNU        11.37      7.56  14.41     1.86  51.10   15.06   44.99    4.82   \n",
       "BARRAS       11.33      6.97  14.09     1.95  49.48   14.48   42.10    4.72   \n",
       "NOOL         11.33      7.27  12.68     1.98  49.20   15.29   37.92    4.62   \n",
       "BOURGUIGNON  11.36      6.80  13.46     1.86  51.16   15.67   40.49    5.02   \n",
       "\n",
       "             Javelot   1500m  \n",
       "Sebrle         70.52  280.01  \n",
       "Clay           69.71  282.00  \n",
       "Karpov         55.54  278.11  \n",
       "Macey          58.46  265.42  \n",
       "Warners        55.39  278.05  \n",
       "Zsivoczky      63.45  269.54  \n",
       "Hernu          57.76  264.35  \n",
       "Nool           61.33  276.33  \n",
       "Bernard        55.27  276.31  \n",
       "Schwarzl       56.32  273.56  \n",
       "Pogorelov      53.45  287.63  \n",
       "Schoenbeck     60.89  278.82  \n",
       "Barras         64.55  267.09  \n",
       "Smith          61.52  272.74  \n",
       "Averyanov      54.51  271.02  \n",
       "Ojaniemi       59.26  275.71  \n",
       "Smirnov        60.88  263.31  \n",
       "Qi             60.79  272.63  \n",
       "Drews          51.53  274.21  \n",
       "Parkhomenko    65.82  277.94  \n",
       "Terek          50.62  290.36  \n",
       "Gomez          60.71  269.70  \n",
       "Turi           59.34  290.01  \n",
       "Lorenzo        58.36  263.08  \n",
       "Karlivans      52.92  278.67  \n",
       "Korkizoglou    53.05  317.00  \n",
       "Uldal          60.00  281.70  \n",
       "Casarsa        58.62  296.12  \n",
       "SEBRLE         63.19  291.70  \n",
       "CLAY           60.15  301.50  \n",
       "KARPOV         50.31  300.20  \n",
       "BERNARD        62.77  280.10  \n",
       "YURKOV         63.44  276.40  \n",
       "WARNERS        51.77  278.10  \n",
       "ZSIVOCZKY      55.37  268.00  \n",
       "McMULLEN       56.37  285.10  \n",
       "MARTINEAU      52.33  262.10  \n",
       "HERNU          57.19  285.10  \n",
       "BARRAS         55.40  282.00  \n",
       "NOOL           57.44  266.60  \n",
       "BOURGUIGNON    54.68  291.70  "
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = decathlon.iloc[:,0:10]\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on _iLocIndexer in module pandas.core.indexing object:\n",
      "\n",
      "class _iLocIndexer(_LocationIndexer)\n",
      " |  Purely integer-location based indexing for selection by position.\n",
      " |  \n",
      " |  ``.iloc[]`` is primarily integer position based (from ``0`` to\n",
      " |  ``length-1`` of the axis), but may also be used with a boolean\n",
      " |  array.\n",
      " |  \n",
      " |  Allowed inputs are:\n",
      " |  \n",
      " |  - An integer, e.g. ``5``.\n",
      " |  - A list or array of integers, e.g. ``[4, 3, 0]``.\n",
      " |  - A slice object with ints, e.g. ``1:7``.\n",
      " |  - A boolean array.\n",
      " |  - A ``callable`` function with one argument (the calling Series, DataFrame\n",
      " |    or Panel) and that returns valid output for indexing (one of the above)\n",
      " |  \n",
      " |  ``.iloc`` will raise ``IndexError`` if a requested indexer is\n",
      " |  out-of-bounds, except *slice* indexers which allow out-of-bounds\n",
      " |  indexing (this conforms with python/numpy *slice* semantics).\n",
      " |  \n",
      " |  See more at :ref:`Selection by Position <indexing.integer>`\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      _iLocIndexer\n",
      " |      _LocationIndexer\n",
      " |      _NDFrameIndexer\n",
      " |      pandas._libs.indexing._NDFrameIndexerBase\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods inherited from _LocationIndexer:\n",
      " |  \n",
      " |  __getitem__(self, key)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from _NDFrameIndexer:\n",
      " |  \n",
      " |  __call__(self, axis=None)\n",
      " |      Call self as a function.\n",
      " |  \n",
      " |  __iter__(self)\n",
      " |  \n",
      " |  __setitem__(self, key, value)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from _NDFrameIndexer:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes inherited from _NDFrameIndexer:\n",
      " |  \n",
      " |  axis = None\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from pandas._libs.indexing._NDFrameIndexerBase:\n",
      " |  \n",
      " |  __init__(self, /, *args, **kwargs)\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  __reduce__ = __reduce_cython__(...)\n",
      " |  \n",
      " |  __setstate__ = __setstate_cython__(...)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Static methods inherited from pandas._libs.indexing._NDFrameIndexerBase:\n",
      " |  \n",
      " |  __new__(*args, **kwargs) from builtins.type\n",
      " |      Create and return a new object.  See help(type) for accurate signature.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from pandas._libs.indexing._NDFrameIndexerBase:\n",
      " |  \n",
      " |  name\n",
      " |  \n",
      " |  ndim\n",
      " |  \n",
      " |  obj\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(decathlon.iloc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class StandardScaler in module sklearn.preprocessing.data:\n",
      "\n",
      "class StandardScaler(sklearn.base.BaseEstimator, sklearn.base.TransformerMixin)\n",
      " |  StandardScaler(copy=True, with_mean=True, with_std=True)\n",
      " |  \n",
      " |  Standardize features by removing the mean and scaling to unit variance\n",
      " |  \n",
      " |  Centering and scaling happen independently on each feature by computing\n",
      " |  the relevant statistics on the samples in the training set. Mean and\n",
      " |  standard deviation are then stored to be used on later data using the\n",
      " |  `transform` method.\n",
      " |  \n",
      " |  Standardization of a dataset is a common requirement for many\n",
      " |  machine learning estimators: they might behave badly if the\n",
      " |  individual feature do not more or less look like standard normally\n",
      " |  distributed data (e.g. Gaussian with 0 mean and unit variance).\n",
      " |  \n",
      " |  For instance many elements used in the objective function of\n",
      " |  a learning algorithm (such as the RBF kernel of Support Vector\n",
      " |  Machines or the L1 and L2 regularizers of linear models) assume that\n",
      " |  all features are centered around 0 and have variance in the same\n",
      " |  order. If a feature has a variance that is orders of magnitude larger\n",
      " |  that others, it might dominate the objective function and make the\n",
      " |  estimator unable to learn from other features correctly as expected.\n",
      " |  \n",
      " |  This scaler can also be applied to sparse CSR or CSC matrices by passing\n",
      " |  `with_mean=False` to avoid breaking the sparsity structure of the data.\n",
      " |  \n",
      " |  Read more in the :ref:`User Guide <preprocessing_scaler>`.\n",
      " |  \n",
      " |  Parameters\n",
      " |  ----------\n",
      " |  copy : boolean, optional, default True\n",
      " |      If False, try to avoid a copy and do inplace scaling instead.\n",
      " |      This is not guaranteed to always work inplace; e.g. if the data is\n",
      " |      not a NumPy array or scipy.sparse CSR matrix, a copy may still be\n",
      " |      returned.\n",
      " |  \n",
      " |  with_mean : boolean, True by default\n",
      " |      If True, center the data before scaling.\n",
      " |      This does not work (and will raise an exception) when attempted on\n",
      " |      sparse matrices, because centering them entails building a dense\n",
      " |      matrix which in common use cases is likely to be too large to fit in\n",
      " |      memory.\n",
      " |  \n",
      " |  with_std : boolean, True by default\n",
      " |      If True, scale the data to unit variance (or equivalently,\n",
      " |      unit standard deviation).\n",
      " |  \n",
      " |  Attributes\n",
      " |  ----------\n",
      " |  scale_ : ndarray, shape (n_features,)\n",
      " |      Per feature relative scaling of the data.\n",
      " |  \n",
      " |      .. versionadded:: 0.17\n",
      " |         *scale_*\n",
      " |  \n",
      " |  mean_ : array of floats with shape [n_features]\n",
      " |      The mean value for each feature in the training set.\n",
      " |  \n",
      " |  var_ : array of floats with shape [n_features]\n",
      " |      The variance for each feature in the training set. Used to compute\n",
      " |      `scale_`\n",
      " |  \n",
      " |  n_samples_seen_ : int\n",
      " |      The number of samples processed by the estimator. Will be reset on\n",
      " |      new calls to fit, but increments across ``partial_fit`` calls.\n",
      " |  \n",
      " |  Examples\n",
      " |  --------\n",
      " |  >>> from sklearn.preprocessing import StandardScaler\n",
      " |  >>>\n",
      " |  >>> data = [[0, 0], [0, 0], [1, 1], [1, 1]]\n",
      " |  >>> scaler = StandardScaler()\n",
      " |  >>> print(scaler.fit(data))\n",
      " |  StandardScaler(copy=True, with_mean=True, with_std=True)\n",
      " |  >>> print(scaler.mean_)\n",
      " |  [ 0.5  0.5]\n",
      " |  >>> print(scaler.transform(data))\n",
      " |  [[-1. -1.]\n",
      " |   [-1. -1.]\n",
      " |   [ 1.  1.]\n",
      " |   [ 1.  1.]]\n",
      " |  >>> print(scaler.transform([[2, 2]]))\n",
      " |  [[ 3.  3.]]\n",
      " |  \n",
      " |  See also\n",
      " |  --------\n",
      " |  scale: Equivalent function without the estimator API.\n",
      " |  \n",
      " |  :class:`sklearn.decomposition.PCA`\n",
      " |      Further removes the linear correlation across features with 'whiten=True'.\n",
      " |  \n",
      " |  Notes\n",
      " |  -----\n",
      " |  For a comparison of the different scalers, transformers, and normalizers,\n",
      " |  see :ref:`examples/preprocessing/plot_all_scaling.py\n",
      " |  <sphx_glr_auto_examples_preprocessing_plot_all_scaling.py>`.\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      StandardScaler\n",
      " |      sklearn.base.BaseEstimator\n",
      " |      sklearn.base.TransformerMixin\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, copy=True, with_mean=True, with_std=True)\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  fit(self, X, y=None)\n",
      " |      Compute the mean and std to be used for later scaling.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : {array-like, sparse matrix}, shape [n_samples, n_features]\n",
      " |          The data used to compute the mean and standard deviation\n",
      " |          used for later scaling along the features axis.\n",
      " |      \n",
      " |      y : Passthrough for ``Pipeline`` compatibility.\n",
      " |  \n",
      " |  inverse_transform(self, X, copy=None)\n",
      " |      Scale back the data to the original representation\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like, shape [n_samples, n_features]\n",
      " |          The data used to scale along the features axis.\n",
      " |      copy : bool, optional (default: None)\n",
      " |          Copy the input X or not.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      X_tr : array-like, shape [n_samples, n_features]\n",
      " |          Transformed array.\n",
      " |  \n",
      " |  partial_fit(self, X, y=None)\n",
      " |      Online computation of mean and std on X for later scaling.\n",
      " |      All of X is processed as a single batch. This is intended for cases\n",
      " |      when `fit` is not feasible due to very large number of `n_samples`\n",
      " |      or because X is read from a continuous stream.\n",
      " |      \n",
      " |      The algorithm for incremental mean and std is given in Equation 1.5a,b\n",
      " |      in Chan, Tony F., Gene H. Golub, and Randall J. LeVeque. \"Algorithms\n",
      " |      for computing the sample variance: Analysis and recommendations.\"\n",
      " |      The American Statistician 37.3 (1983): 242-247:\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : {array-like, sparse matrix}, shape [n_samples, n_features]\n",
      " |          The data used to compute the mean and standard deviation\n",
      " |          used for later scaling along the features axis.\n",
      " |      \n",
      " |      y : Passthrough for ``Pipeline`` compatibility.\n",
      " |  \n",
      " |  transform(self, X, y='deprecated', copy=None)\n",
      " |      Perform standardization by centering and scaling\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like, shape [n_samples, n_features]\n",
      " |          The data used to scale along the features axis.\n",
      " |      y : (ignored)\n",
      " |          .. deprecated:: 0.19\n",
      " |             This parameter will be removed in 0.21.\n",
      " |      copy : bool, optional (default: None)\n",
      " |          Copy the input X or not.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.base.BaseEstimator:\n",
      " |  \n",
      " |  __getstate__(self)\n",
      " |  \n",
      " |  __repr__(self)\n",
      " |      Return repr(self).\n",
      " |  \n",
      " |  __setstate__(self, state)\n",
      " |  \n",
      " |  get_params(self, deep=True)\n",
      " |      Get parameters for this estimator.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      deep : boolean, optional\n",
      " |          If True, will return the parameters for this estimator and\n",
      " |          contained subobjects that are estimators.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      params : mapping of string to any\n",
      " |          Parameter names mapped to their values.\n",
      " |  \n",
      " |  set_params(self, **params)\n",
      " |      Set the parameters of this estimator.\n",
      " |      \n",
      " |      The method works on simple estimators as well as on nested objects\n",
      " |      (such as pipelines). The latter have parameters of the form\n",
      " |      ``<component>__<parameter>`` so that it's possible to update each\n",
      " |      component of a nested object.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from sklearn.base.BaseEstimator:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.base.TransformerMixin:\n",
      " |  \n",
      " |  fit_transform(self, X, y=None, **fit_params)\n",
      " |      Fit to data, then transform it.\n",
      " |      \n",
      " |      Fits transformer to X and y with optional parameters fit_params\n",
      " |      and returns a transformed version of X.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : numpy array of shape [n_samples, n_features]\n",
      " |          Training set.\n",
      " |      \n",
      " |      y : numpy array of shape [n_samples]\n",
      " |          Target values.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      X_new : numpy array of shape [n_samples, n_features_new]\n",
      " |          Transformed array.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(StandardScaler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler(with_mean = True)\n",
    "Y = scaler.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y.mean_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on built-in function mean:\n",
      "\n",
      "mean(...) method of numpy.ndarray instance\n",
      "    a.mean(axis=None, dtype=None, out=None, keepdims=False)\n",
      "    \n",
      "    Returns the average of the array elements along given axis.\n",
      "    \n",
      "    Refer to `numpy.mean` for full documentation.\n",
      "    \n",
      "    See Also\n",
      "    --------\n",
      "    numpy.mean : equivalent function\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(Y.mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 6.66675387e-15, -2.33417621e-15, -1.51640218e-15, -1.67887384e-16,\n",
       "       -7.27331474e-15, -2.80534403e-15, -7.41953924e-16, -8.85470559e-16,\n",
       "        2.12025519e-15, -2.87574842e-15])"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y.mean(axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# matrice des variances des differentes colonnes \n",
    "S= Y.var(axis = 0)\n",
    "S\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.02439024, 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.        , 0.02439024, 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.        , 0.        , 0.02439024, ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       ...,\n",
       "       [0.        , 0.        , 0.        , ..., 0.02439024, 0.        ,\n",
       "        0.        ],\n",
       "       [0.        , 0.        , 0.        , ..., 0.        , 0.02439024,\n",
       "        0.        ],\n",
       "       [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "        0.02439024]])"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n = X.shape[0]\n",
    "D = np.diag(np.ones(n)) \n",
    "D = D/n\n",
    "D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 1., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]])"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "M = np.diag(1/np.sqrt(S))\n",
    "M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.        , -0.59867767, -0.35648227, -0.24625292,  0.52029815,\n",
       "         0.57988893, -0.22170757, -0.08253683, -0.15774645, -0.06054645],\n",
       "       [-0.59867767,  1.        ,  0.18330436,  0.29464444, -0.60206262,\n",
       "        -0.50541009,  0.19431009,  0.20401411,  0.11975893, -0.03368613],\n",
       "       [-0.35648227,  0.18330436,  1.        ,  0.48921153, -0.13843292,\n",
       "        -0.25161571,  0.6157681 ,  0.06118185,  0.37495551,  0.11580306],\n",
       "       [-0.24625292,  0.29464444,  0.48921153,  1.        , -0.18795693,\n",
       "        -0.28328909,  0.36921834, -0.15618074,  0.17188009, -0.04490252],\n",
       "       [ 0.52029815, -0.60206262, -0.13843292, -0.18795693,  1.        ,\n",
       "         0.54798776, -0.11787936, -0.07929247,  0.0042321 ,  0.40810643],\n",
       "       [ 0.57988893, -0.50541009, -0.25161571, -0.28328909,  0.54798776,\n",
       "         1.        , -0.32620096, -0.00270388,  0.00874325,  0.03754024],\n",
       "       [-0.22170757,  0.19431009,  0.6157681 ,  0.36921834, -0.11787936,\n",
       "        -0.32620096,  1.        , -0.1500724 ,  0.1578898 ,  0.2581751 ],\n",
       "       [-0.08253683,  0.20401411,  0.06118185, -0.15618074, -0.07929247,\n",
       "        -0.00270388, -0.1500724 ,  1.        , -0.0300006 ,  0.24744778],\n",
       "       [-0.15774645,  0.11975893,  0.37495551,  0.17188009,  0.0042321 ,\n",
       "         0.00874325,  0.1578898 , -0.0300006 ,  1.        , -0.18039313],\n",
       "       [-0.06054645, -0.03368613,  0.11580306, -0.04490252,  0.40810643,\n",
       "         0.03754024,  0.2581751 ,  0.24744778, -0.18039313,  1.        ]])"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "V=Y.T.dot(D).dot(Y)\n",
    "V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.        , -0.59867767, -0.35648227, -0.24625292,  0.52029815,\n",
       "         0.57988893, -0.22170757, -0.08253683, -0.15774645, -0.06054645],\n",
       "       [-0.59867767,  1.        ,  0.18330436,  0.29464444, -0.60206262,\n",
       "        -0.50541009,  0.19431009,  0.20401411,  0.11975893, -0.03368613],\n",
       "       [-0.35648227,  0.18330436,  1.        ,  0.48921153, -0.13843292,\n",
       "        -0.25161571,  0.6157681 ,  0.06118185,  0.37495551,  0.11580306],\n",
       "       [-0.24625292,  0.29464444,  0.48921153,  1.        , -0.18795693,\n",
       "        -0.28328909,  0.36921834, -0.15618074,  0.17188009, -0.04490252],\n",
       "       [ 0.52029815, -0.60206262, -0.13843292, -0.18795693,  1.        ,\n",
       "         0.54798776, -0.11787936, -0.07929247,  0.0042321 ,  0.40810643],\n",
       "       [ 0.57988893, -0.50541009, -0.25161571, -0.28328909,  0.54798776,\n",
       "         1.        , -0.32620096, -0.00270388,  0.00874325,  0.03754024],\n",
       "       [-0.22170757,  0.19431009,  0.6157681 ,  0.36921834, -0.11787936,\n",
       "        -0.32620096,  1.        , -0.1500724 ,  0.1578898 ,  0.2581751 ],\n",
       "       [-0.08253683,  0.20401411,  0.06118185, -0.15618074, -0.07929247,\n",
       "        -0.00270388, -0.1500724 ,  1.        , -0.0300006 ,  0.24744778],\n",
       "       [-0.15774645,  0.11975893,  0.37495551,  0.17188009,  0.0042321 ,\n",
       "         0.00874325,  0.1578898 , -0.0300006 ,  1.        , -0.18039313],\n",
       "       [-0.06054645, -0.03368613,  0.11580306, -0.04490252,  0.40810643,\n",
       "         0.03754024,  0.2581751 ,  0.24744778, -0.18039313,  1.        ]])"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "VM = V.dot(M)\n",
    "VM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TROUVER LES VALEURS PROPRES ET LES VECTEURS PROPRES DE VM et normalisez les vecteurs propres \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "constructeur\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt \n",
    "\n",
    "## Ma class ACP \n",
    "class MyACP : \n",
    "     \n",
    "      def __init__(self, X: pd.DataFrame, D : np.array = None , M : np.array = None ) :  \n",
    "        dim = X.shape\n",
    "        n = dim[0]\n",
    "        p = dim[1]\n",
    "        \n",
    "        #Donnes centres\n",
    "        self.Y = StandardScaler(with_mean = True , with_std = False).fit_transform(X)\n",
    "        \n",
    "        #matrice des poids \n",
    "        if D is None : \n",
    "            self.D = np.diag((1/n)*np.ones(n))\n",
    "        else :\n",
    "            self.D = D.copy()\n",
    "            \n",
    "        \n",
    "        #Variances covariances \n",
    "        self.V = Y.dot(D).dot(Y)\n",
    "        \n",
    "        #Metrique\n",
    "        \n",
    "        if M is not None :\n",
    "            self.M = M.copy()\n",
    "        else : \n",
    "            self.M= np.diag(1/n.diag(self.V))\n",
    "        \n",
    "        #VM \n",
    "        VM = self.V.dot(self.M)\n",
    "        \n",
    "        #Decomposition spectrale \n",
    "        val_propres , vect_propres = np.linalg.eig(VM)\n",
    "            \n",
    "acp = MyACP()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
